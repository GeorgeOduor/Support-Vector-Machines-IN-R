{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <center> SUPPORT VECTOR MACHINES </center>\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Introduction\n",
    "1. Understanding SVMs\n",
    "    1. Maximal margin Classifier\n",
    "        1. A Hyperplane\n",
    "        1. Classification using a separating hyperplane.\n",
    "        1. The Maximal Margin Classifier.\n",
    "    1. Support Vector Classifiers.\n",
    "    1. Pros and Cons of SVM.\n",
    "        1. Pros\n",
    "        1. Cons\n",
    "    1. SVM Applications.\n",
    "1. Implementation with R-Brest Cancer Prediction.\n",
    "    1.Kernel Link\n",
    "    \n",
    "# Introduction.\n",
    "\n",
    "A support vector machine is a supervised algorithm that classifies cases by finding a separator.It works by first mapping data to a high dimensional feature space so that data can be classified even if the data is not linearly sepparable then it finds a separator.\n",
    "\n",
    "# Understanding SVMs.\n",
    "\n",
    "To understand support vector machines its important to understand some concepts of **Maximal masrgin classifier** and **hyperplanes.**\n",
    "\n",
    "## Maximal Margin Classifier.\n",
    "\n",
    "### A Hyperplane.\n",
    "\n",
    "A hyperplane in svms is a separating line or plane in a 2 dimensional or 3 dimensional space respectively.\n",
    "\n",
    "Mathematicaly,in a 2D space it is defined by the equation:\n",
    "\n",
    "$$\\beta_o+\\beta_1X_1+\\beta_2X_2 = 0$$\n",
    "\n",
    "for the beta parameters.A training data point  that satisfies the above equation is apoint on the line otherwise the point fall below or above the line. In the case of a p dimensional space the equation will be something like.\n",
    "\n",
    "$$\\beta_o+\\beta_1X_1+\\beta_2X_2 + ... + \\beta_pX_p= 0$$\n",
    "\n",
    "If X is a data point in the training set and doest satisfy the equation above,its equation will be :\n",
    "\n",
    "$$\\beta_o+\\beta_1X_1+\\beta_2X_2 + ... + \\beta_pX_p > 0$$ if X lies above the hyperplane and \n",
    "\n",
    "$$\\beta_o+\\beta_1X_1+\\beta_2X_2 + ... + \\beta_pX_p < 0$$\n",
    "\n",
    "if X lies below the hyperplane.\n",
    "\n",
    "### Classification using a separating hyperplane.\n",
    "\n",
    "If it is possible to find a separating plane that devides datapoints into sepate classes based on their class labels,then this separating plane must have the property below:\n",
    "\n",
    "$$y_i(\\beta_o+\\beta_1X_1+\\beta_2X_2 + ... + \\beta_pX_p) < 0$$\n",
    "$$y_i\\in \\{-1,1\\},\\forall \\ i =1 ... n$$\n",
    "\n",
    "This forms the decision rule.A larger X value in in the datapoints shows that its far from 0 and thus far from the hyperplane and can be classified easily.Points closser to 0 are more closer to the hyperplane and can be hard to classify.Due to this problem,lets examine _the maximal margin  classifier_ concept.\n",
    "\n",
    "### The Maximal Margin Classifier. \n",
    "\n",
    "Practicaly if there is a hyperplane that can separate training data into two classes,then these lines can be so many.This is realised by the fact that this hyperplane can be tilted or even moved severally through tiny distances and still satisfy the conditions outlined above.\n",
    "\n",
    "_Maximal margin hyperplane_(optimal separating hyperplane) helps in determining a reasonable way of deciding on which of the many possible hyperplanes to use.\n",
    "\n",
    "This is done by computing the parpendicular distances between each training point to a particular hyperplane untill the smallest distance is found.This smallest distance is called the margin.The hyperplane that has the furthest minimum distance to the training observations is now the maximal margin hyperplane.\n",
    "\n",
    "The test features can then be classified based on which side it falls and this is what is known as **maximal margin classifier.** For a training set with a large number of features can lead to overfitting based on this algorithm.\n",
    "\n",
    "In a simple explanatioon,the maximal margin hyperplane represents the line in the middle of the road and the maximal margin represents the widest street whose midele is represented by the hyperplane.\n",
    "\n",
    "Points that lie along the edge of this margin(street) are known as __support vectors__.They are named so because they literaly support the maximal margin hyperplane because any movement of by them in any direction will cause the hyperplane to shift too.\n",
    "\n",
    "The maximal margin clasifier depends only on the support vectors and any changes on otherpoints will not affect the classification if and only if the movement doesnt cause it to cross the line.\n",
    "\n",
    "\n",
    "\n",
    "## Support Vector Classifiers.\n",
    "\n",
    "As  shown above ,the macimal margin hyperplane will perfectly separate training points.This makes it highly sensitive because when a new varible is added to the training set,the hyperplane shifts drasticaly.This is dangerous.\n",
    "\n",
    "This calls for creating a classifier based on the hyperplane that doesn ot perfectly separate two classes.This is is done in order to achieve:\n",
    "\n",
    "- Greater robustness to individual observations\n",
    "- Better classsification of most of the training observations.\n",
    "\n",
    "The idea here is that it is meaningfull to classify a few training observations wrongly in order to classify as many as posible in the remaining the observations.\n",
    "\n",
    "This is achieved by the _support vector classifier._ \n",
    "\n",
    "$maximize M$\n",
    "\n",
    "subject to $\\sum_{j=1}^p\\beta_j^2=1,$\n",
    "\n",
    "$y_i(\\beta_o+\\beta_1X_1+\\beta_2X_2 + ... + \\beta_pX_p)\\geq M(1-\\epsilon_i)$\n",
    "\n",
    "$\\epsilon_i\\neq0,\\sum_{1=1}^n\\epsilon_i\\leq C$ \n",
    "\n",
    "## Support Vector Machines.\n",
    "\n",
    "Support vector machine is an extension of the support vector clasiifier that results from enlarging the feature space using kernels.This is done to accomodate a non linear boundary between classes.\n",
    "\n",
    "## Pros and Cons of SVM.\n",
    "\n",
    "### Pros.\n",
    "\n",
    "1. Accurate in high dimensional spaces.\n",
    "\n",
    "1. **Memory efficient** because they rely ony on a sybset of training observations called support vectors.\n",
    "\n",
    "### Cons\n",
    "\n",
    "1. Prone to overfitting of the number of features is greater than the number of samples.\n",
    "\n",
    "1. No probabability estimation which is desirabel in most classification processes.\n",
    "\n",
    "1. Not efficient if the data is large.\n",
    "\n",
    "## SVM Applications.\n",
    "\n",
    "- Image classification.\n",
    "\n",
    "- Text mining \n",
    "\n",
    "- Gene expression classification\n",
    "\n",
    "- Sentiment analysis\n",
    "\n",
    "- Spam detection\n",
    "\n",
    "- Regression outlier detection and clustering.\n",
    "\n",
    "# Implementation with R-Brest Cancer Prediction.\n",
    "\n",
    "Lets put theory into practice by applying the SVM algorithm to predict if a patient cancer condition will relapse or not..\n",
    "\n",
    "\n",
    "### Find the kernel here.\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
